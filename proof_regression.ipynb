{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43be83a",
   "metadata": {},
   "source": [
    "# **2/2568 FRA501: Pattern Recognition**\n",
    "## **HW1 Clustering and Regression**\n",
    "\n",
    "**Member**\n",
    "- 65340500037 Pavaris Asawakijtananont\n",
    "- 65340500058 Anuwit Intet\n",
    "- 65340500062 Aitthikit Kitchareonnon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbc44b",
   "metadata": {},
   "source": [
    "Consider a supervised learning problem with $n$ training samples, feature vectors $x_i \\in \\mathbb{R}^d$, target values $y_i \\in \\mathbb{R}$, and a linear model $\\hat{y}_i = x_i^\\top \\theta$, where $\\theta \\in \\mathbb{R}^d$; the loss function is defined as:\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} |x_i^\\top \\theta - y_i| + \\lambda \\|\\theta\\|_2^2, \\quad \\lambda > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a19962",
   "metadata": {},
   "source": [
    "### **3.1. Derive the gradient of $L(\\theta)$. (5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c97beb",
   "metadata": {},
   "source": [
    "We will divide the differentiation into two parts:\n",
    "\n",
    "##### **1. Mean Absolute Error Term**\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} |x_i^\\top \\theta - y_i|$$\n",
    "\n",
    "Finding the gradient of the first term can be done using the following steps:\n",
    "\n",
    "1. Since gradients have linearity, we can directly place the $\\nabla_\\theta$ operator after the sum sign ($\\sum$):\n",
    "$$\\nabla_\\theta f(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_\\theta |x_i^\\top \\theta - y_i|$$\n",
    "\n",
    "2. Using the basic calculus formula $\\frac{d}{du}|u| = \\text{sign}(u)$ or $\\frac{u}{|u|}$\n",
    "Given $u = x_i^\\top \\theta - y_i$, we have:\n",
    "$$\\nabla_\\theta |u| = \\text{sign}(u) \\cdot \\nabla_\\theta(u)$$\n",
    "\n",
    "3. Differentiate the inner term $u = (x_i^\\top \\theta - y_i)$ with respect to the vector $\\theta$:\n",
    "\n",
    "    * $\\nabla_\\theta (x_i^\\top \\theta) = x_i$\n",
    "\n",
    "    * $\\nabla_\\theta (y_i) = 0$\n",
    "\n",
    "When all the elements are put together, the gradient of the first term is:\n",
    "\n",
    "$$\\nabla_\\theta \\left( \\frac{1}{n} \\sum_{i=1}^{n} |x_i^\\top \\theta - y_i| \\right) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sign}(x_i^\\top \\theta - y_i) \\cdot x_i$$\n",
    "\n",
    "##### **2. $L_2$ Regularization Term** \n",
    "\n",
    "$$\\lambda \\|\\theta\\|_2^2$$\n",
    "\n",
    "The gradient of the second term can be found by considering the partial derivative as follows:\n",
    "\n",
    "* From the definition $\\|\\theta\\|_2^2 = \\left( \\sqrt{\\theta_1^2 + \\theta_2^2 + \\dots + \\theta_j^2 + \\dots + \\theta_m^2} \\right)^2 = \\theta_1^2 + \\theta_2^2 + \\dots + \\theta_j^2 + \\dots + \\theta_m^2$\n",
    "* Therefore: $\\frac{\\partial}{\\partial \\theta_j} (\\lambda \\sum_{i=1}^{m} \\theta_i^2) = \\lambda \\cdot (2\\theta_j) = 2\\lambda\\theta_j$\n",
    "\n",
    "Since:\n",
    "$$\\nabla_\\theta L(\\theta) = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\theta_1} \\\\ \\frac{\\partial L}{\\partial \\theta_2} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial \\theta_m} \\end{bmatrix}$$\n",
    "\n",
    "When the differential results ($2\\lambda\\theta_1, 2\\lambda\\theta_2, \\dots$) are arranged contiguously in a vector:\n",
    "$$\\begin{bmatrix} 2\\lambda\\theta_1 \\\\ 2\\lambda\\theta_2 \\\\ \\vdots \\\\ 2\\lambda\\theta_m \\end{bmatrix} = 2\\lambda \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_m \\end{bmatrix} = 2\\lambda\\theta$$\n",
    "\n",
    "When formatted nicely, the gradient of the second term will be:\n",
    "$$\\nabla_\\theta (\\lambda \\|\\theta\\|_2^2) = 2\\lambda\\theta$$\n",
    "\n",
    "When parts 1 and 2 are added together according to the law of differentiation, the final solution of $\\nabla_\\theta L(\\theta)$ is:\n",
    "\n",
    "$$\\nabla_\\theta L(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sign}(x_i^\\top \\theta - y_i) x_i + 2\\lambda\\theta$$\n",
    "\n",
    "- n is the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffbf3b",
   "metadata": {},
   "source": [
    "Proof of Derivertive of Absolute Value: https://youtu.be/76SrD5P5V-0?si=eHcZp2XoIsbgH0jk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da2c6f",
   "metadata": {},
   "source": [
    "### **3.2. Using the result from (3.1), write the gradient descent update rule for $\\theta$, assuming a learning rate $\\eta$. (2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d207a7c",
   "metadata": {},
   "source": [
    "$$\\theta_j = \\theta_j - \\eta \\cdot \\nabla_\\theta L(\\theta)$$\n",
    "$$\\theta_j = \\theta_j - \\eta \\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\text{sign}(x_i^\\top \\theta - y_i) \\cdot x_{i,j} + 2\\lambda\\theta_j \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669510bd",
   "metadata": {},
   "source": [
    "### **3.3. Explain the role of the regularization term $\\lambda \\|\\theta\\|_2^2$ in the update rule. (3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1fb13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d100833c",
   "metadata": {},
   "source": [
    "### **3.4. Explain how the update rule derived differs from that of Mean Squared Error (MSE) in terms of the modelâ€™s learning behavior. (5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa58b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
